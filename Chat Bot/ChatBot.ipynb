{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdJZ0oLLAr-I",
        "outputId": "6e00fa14-9faf-41ab-e8cf-3456fcdfb4bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> User:Hi\n",
            "DialoGPT: Hi! :D\n",
            ">> User:Who are you?\n",
            "DialoGPT: I'm the guy who's been here for a while.\n",
            ">> User:What can you do?\n",
            "DialoGPT: I can do a lot.\n",
            ">> User:Can you be my ML Lab Experiment?\n",
            "DialoGPT: I can try.\n",
            ">> User:Just for one Experiment, Thanks in Advance\n",
            "DialoGPT: I'll try to do it.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import logging\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "for step in range(5):\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n"
      ]
    }
  ]
}